{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T22:02:21.518241Z",
     "start_time": "2025-03-15T22:01:51.289154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from typing import Dict, List, Union\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# ------------------ Configuration ------------------\n",
    "DEEPSEEK_API_KEY = \"sk-02eec7cecae3429facd43a26fc0ab060\"\n",
    "CSV_PATH = \"Code/cleaned_dataset.csv\"\n",
    "OUTPUT_JSON = \"enhanced_structured_events.json\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "MAX_WORKERS = 3\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ------------------ Pydantic Models ------------------\n",
    "class EventSchema(BaseModel):\n",
    "    event_id: str = Field(..., example=\"E20231005_01\")\n",
    "    timestamp: str = Field(..., description=\"ISO8601 timestamp\")\n",
    "    event_type: str = Field(..., examples=[\"Social Protest\", \"Policy Change\", \"M&A\"])\n",
    "    description: str = Field(..., description=\"Subject-Action-Object format\")\n",
    "    source: str = Field(..., examples=[\"News Media\", \"Government Report\", \"Social Media\"])\n",
    "    location: str = Field(..., examples=[\"China/Beijing\", \"USA/New York\"])\n",
    "    impact_industries: List[str] = Field(..., examples=[\"Finance\", \"Energy\"])\n",
    "    summary: str = Field(..., max_length=150, description=\"Concise event summary\")\n",
    "    explanation: List[str] = Field(..., description=\"Step-by-step reasoning process\")\n",
    "\n",
    "class InterpretabilitySchema(BaseModel):\n",
    "    event_id: str = Field(...)\n",
    "    time_window: str = Field(..., examples=[\"T+1d\", \"T+7d\"])\n",
    "    causal_strength: Union[float, str] = Field(..., examples=[0.8, \"High\"])\n",
    "    reasoning_path: str = Field(..., example=\"Policy Change → Regulatory Impact → Market Response\")\n",
    "    impact_summary: str = Field(..., max_length=150, description=\"Impact summary\")\n",
    "\n",
    "class StructuredResponse(BaseModel):\n",
    "    event_table: Union[List[EventSchema], EventSchema]\n",
    "    interpretability_table: Union[List[InterpretabilitySchema], InterpretabilitySchema]\n",
    "\n",
    "# ------------------ Enhanced Error Handler ------------------\n",
    "class EnhancedErrorHandler:\n",
    "    def __init__(self):\n",
    "        self.error_log = []\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def add_error(self, text: str, error_type: str, details: str = \"\"):\n",
    "        with self.lock:\n",
    "            error_entry = {\n",
    "                \"error_type\": error_type,\n",
    "                \"details\": details,\n",
    "                \"original_text\": text[:500]\n",
    "            }\n",
    "            self.error_log.append(error_entry)\n",
    "            return error_entry\n",
    "\n",
    "# ------------------ Optimized Processor ------------------\n",
    "class DeepSeekProcessor:\n",
    "    def __init__(self):\n",
    "        self.error_handler = EnhancedErrorHandler()\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        self.system_prompt = \"\"\"You are a expert analyst. Generate structured data in ENGLISH with:\n",
    "        {schema_template}\n",
    "        \n",
    "        Requirements:\n",
    "        1. Event ID format: E_YYYYMMDD_XX (e.g. E20231005_01)\n",
    "        2. Time window format: T+[number][unit] (e.g. T+7d)\n",
    "        3. Causal strength: 0-1 value preferred\n",
    "        4. Impact industries: [Finance, Energy, Tech, Manufacturing, Real Estate]\n",
    "        5. Summary: 1-sentence key points\n",
    "        6. Explanation: 3-5 stepwise reasoning points\"\"\"\n",
    "        \n",
    "        self.user_prompt = \"\"\"Analyze this text in Chinese and generate structured data in ENGLISH:\n",
    "        {text}\n",
    "        \n",
    "        Output Requirements:\n",
    "        - Event description MUST use Subject-Verb-Object format\n",
    "        - Each event must have ≥1 interpretability record\n",
    "        - Maintain ID consistency between tables\n",
    "        - Summaries should capture core elements\n",
    "        - Explanations show multi-factor analysis\"\"\"\n",
    "\n",
    "    def generate_payload(self, text: str) -> dict:\n",
    "        schema_template = json.dumps(StructuredResponse.schema(), indent=2)\n",
    "        return {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt.format(schema_template=schema_template)},\n",
    "                {\"role\": \"user\", \"content\": self.user_prompt.format(text=text)}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 2500\n",
    "        }\n",
    "\n",
    "    def call_api_with_retry(self, text: str, max_retries=3) -> Union[dict, None]:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.post(\n",
    "                    API_URL,\n",
    "                    headers=headers,\n",
    "                    json=self.generate_payload(text),\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return self.parse_response(response.json()['choices'][0]['message']['content'])\n",
    "            except requests.exceptions.Timeout:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                self.error_handler.add_error(text, \"API_TIMEOUT\", f\"Max retries {max_retries}\")\n",
    "            except Exception as e:\n",
    "                self.error_handler.add_error(text, \"API_ERROR\", str(e))\n",
    "        return None\n",
    "\n",
    "    def parse_response(self, response_text: str) -> Dict:\n",
    "        try:\n",
    "            clean_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            raw_data = json.loads(clean_text)\n",
    "            \n",
    "            # Normalize data structure\n",
    "            if isinstance(raw_data.get(\"event_table\"), dict):\n",
    "                raw_data[\"event_table\"] = [raw_data[\"event_table\"]]\n",
    "            if isinstance(raw_data.get(\"interpretability_table\"), dict):\n",
    "                raw_data[\"interpretability_table\"] = [raw_data[\"interpretability_table\"]]\n",
    "            \n",
    "            validated = StructuredResponse(**raw_data)\n",
    "            return validated.dict()\n",
    "        except (json.JSONDecodeError, ValidationError) as e:\n",
    "            self.error_handler.add_error(response_text, \"PARSE_ERROR\", str(e))\n",
    "            return {}\n",
    "\n",
    "# ------------------ Processing Pipeline ------------------\n",
    "def process_row(processor, row):\n",
    "    try:\n",
    "        text = str(row['Content'])\n",
    "        response = processor.call_api_with_retry(text)\n",
    "        if not response:\n",
    "            return {}\n",
    "            \n",
    "        # Add processing metadata\n",
    "        response[\"source_meta\"] = {\n",
    "            \"original_time\": row.get('Time', ''),\n",
    "            \"text_hash\": hashlib.sha256(text.encode()).hexdigest()[:32],\n",
    "            \"raw_text_snippet\": text[:256]\n",
    "        }\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        processor.error_handler.add_error(str(row), \"PROCESS_ERROR\", str(e))\n",
    "        return {}\n",
    "\n",
    "def main_process():\n",
    "    processor = DeepSeekProcessor()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        required_cols = ['Time', 'Content']\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "        df = df.dropna(subset=['Content']).drop_duplicates(subset=['Content'])\n",
    "        print(f\"Successfully loaded {len(df)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Initialization failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(process_row, processor, row) for _, row in df.iterrows()]\n",
    "        \n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            try:\n",
    "                if data := future.result():\n",
    "                    results.append(data)\n",
    "                print(f\"Processed {i+1}/{len(df)}\", end='\\r')\n",
    "            except Exception as e:\n",
    "                processor.error_handler.add_error(\"\", \"THREAD_ERROR\", str(e))\n",
    "\n",
    "    save_results(results, processor.error_handler)\n",
    "\n",
    "def save_results(data: List[Dict], handler: EnhancedErrorHandler):\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"total_events\": sum(len(d.get(\"event_table\", [])) for d in data),\n",
    "            \"total_interpretations\": sum(len(d.get(\"interpretability_table\", [])) for d in data),\n",
    "            \"success_rate\": f\"{len(data)/(len(data)+len(handler.error_log)):.1%}\" if (len(data)+len(handler.error_log)) > 0 else \"0%\",\n",
    "            \"error_stats\": {\n",
    "                \"total_errors\": len(handler.error_log),\n",
    "                \"error_types\": {err[\"error_type\"]: sum(1 for e in handler.error_log if e[\"error_type\"] == err[\"error_type\"]) \n",
    "                              for err in handler.error_log}\n",
    "            }\n",
    "        },\n",
    "        \"data\": data,\n",
    "        \"errors\": handler.error_log\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main_process()\n",
    "    print(f\"\\nTotal processing time: {time.time()-start_time:.2f}s\")\n",
    "    print(f\"Results saved to: {OUTPUT_JSON}\")"
   ],
   "id": "2200ddfb54245888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3 records\n",
      "Processed 3/3\r\n",
      "Total processing time: 30.20s\n",
      "Results saved to: enhanced_structured_events.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab9fc7ce77e502da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
