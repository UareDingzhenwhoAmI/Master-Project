{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:03:33.050568Z",
     "start_time": "2025-03-17T17:09:25.715863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import re\n",
    "from typing import Dict, List, Union\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator  # 修改导入\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------ Configuration ------------------（保持不变）\n",
    "DEEPSEEK_API_KEY = \"sk-02eec7cecae3429facd43a26fc0ab060\"\n",
    "CSV_PATH = \"part1.csv\"\n",
    "OUTPUT_JSON = \"enhanced_structured_events2.json\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "MAX_WORKERS = 3\n",
    "\n",
    "# ------------------ Pydantic Models ------------------\n",
    "class EventSchema(BaseModel):\n",
    "    event_id: str = Field(..., example=\"E20231005_01\")\n",
    "    timestamp: str = Field(..., description=\"ISO8601 timestamp\")\n",
    "    event_type: str = Field(..., examples=[\"Social Protest\", \"Policy Change\", \"M&A\"])\n",
    "    description: str = Field(..., description=\"Subject-Action-Object format\")\n",
    "    source: str = Field(..., examples=[\"News Media\", \"Government Report\", \"Social Media\"])\n",
    "    location: str = Field(..., examples=[\"China/Beijing\", \"USA/New York\"])\n",
    "    impact_industries: List[str] = Field(..., examples=[\"Finance\", \"Energy\"])\n",
    "    summary: str = Field(..., max_length=500, description=\"Concise event summary\")  # 修改为允许最多 500 个字符\n",
    "    explanation: List[str] = Field(..., description=\"Step-by-step reasoning process\")\n",
    "\n",
    "class InterpretabilitySchema(BaseModel):  # 保持不变\n",
    "    event_id: str = Field(...)\n",
    "    time_window: str = Field(..., examples=[\"T+1d\", \"T+7d\"])\n",
    "    causal_strength: Union[float, str] = Field(..., examples=[0.8, \"High\"])\n",
    "    reasoning_path: str = Field(..., example=\"Policy Change → Regulatory Impact → Market Response\")\n",
    "    impact_summary: str = Field(..., max_length=300, description=\"Impact summary\")\n",
    "\n",
    "class EconomicImpactSchema(BaseModel):\n",
    "    event_id: str = Field(...)\n",
    "    indicator: str = Field(..., \n",
    "        examples=[\"GDP\", \"Unemployment Rate\", \"Trade Volume\"],\n",
    "        pattern=r\"^(GDP|Unemployment|Inflation|Trade|FDI|Stock Market|Local GDP|Tourism Revenue|Arts Funding|Retail Sales|Crime Rate|Church Donations|Community Engagement|Volunteer Participation|Real Estate Prices)$\"  # 扩展白名单\n",
    "    )\n",
    "    direction: str = Field(..., examples=[\"Increase\", \"Decrease\"])\n",
    "    magnitude: str = Field(..., examples=[\"2%\", \"0.5B USD\"])\n",
    "    confidence: float = Field(..., ge=0, le=1)\n",
    "    time_horizon: str = Field(..., examples=[\"1 year\", \"3-5 years\"])\n",
    "    affected_areas: List[str] = Field(..., examples=[\"UK\", \"EU\"])\n",
    "\n",
    "    @field_validator('magnitude')  # 修改为field_validator\n",
    "    def validate_magnitude(cls, v: str) -> str:\n",
    "        pattern = r'^[\\d\\.]+-?[\\d\\.]*%?(?:\\s?[A-Za-z\\/]+)*$'  # 增强正则表达式\n",
    "        if not re.match(pattern, v):\n",
    "            raise ValueError(f'Invalid magnitude format: {v}')\n",
    "        return v\n",
    "\n",
    "    @field_validator('time_horizon')\n",
    "    def validate_time_horizon(cls, v: str) -> str:\n",
    "        if not re.match(r'^\\d+(\\s?-\\s?\\d+)?\\s(years?|months?|quarters?|year|month|quarter)$', v):\n",
    "            raise ValueError(f'Invalid time horizon: {v}')\n",
    "        return v\n",
    "\n",
    "class StructuredResponse(BaseModel):  # 保持不变\n",
    "    event_table: Union[List[EventSchema], EventSchema]\n",
    "    interpretability_table: Union[List[InterpretabilitySchema], InterpretabilitySchema]\n",
    "    economic_impact_table: Union[List[EconomicImpactSchema], EconomicImpactSchema]\n",
    "\n",
    "# ------------------ Enhanced Error Handler ------------------（保持不变）\n",
    "class EnhancedErrorHandler:\n",
    "    def __init__(self):\n",
    "        self.error_log = []\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def add_error(self, text: str, error_type: str, details: str = \"\"):\n",
    "        with self.lock:\n",
    "            error_entry = {\n",
    "                \"error_type\": error_type,\n",
    "                \"details\": details,\n",
    "                \"original_text\": text[:500]\n",
    "            }\n",
    "            self.error_log.append(error_entry)\n",
    "            return error_entry\n",
    "\n",
    "# ------------------ Optimized Processor ------------------\n",
    "class DeepSeekProcessor:\n",
    "    def __init__(self):\n",
    "        self.error_handler = EnhancedErrorHandler()\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # 调整系统提示（关键修改）\n",
    "        self.system_prompt = \"\"\"You are an expert analyst. Generate VALID JSON in ENGLISH with:\n",
    "        {schema_template}\n",
    "        \n",
    "        Critical Requirements:\n",
    "        1. Use EXACTLY these economic indicators: \n",
    "           [GDP, Unemployment, Inflation, Trade, FDI, Stock Market, Local GDP, Tourism Revenue, Arts Funding]\n",
    "        2. Magnitude format examples: \"5%\", \"0.5B USD\", \"1.5%\" \n",
    "        3. Time horizon format: \"6 months\", \"2-3 years\"\n",
    "        4. Ensure JSON brackets are properly closed\n",
    "        5. Never truncate JSON output\"\"\"\n",
    "\n",
    "        self.user_prompt = \"\"\"Analyze this Chinese text and generate structured data in ENGLISH:\n",
    "        {text}\n",
    "        \n",
    "        Output MUST:\n",
    "        - Use ONLY the allowed economic indicators\n",
    "        - Format magnitudes exactly like examples\n",
    "        - Keep JSON syntax valid\n",
    "        - Escape special characters\"\"\"\n",
    "\n",
    "    # 增强JSON解析（关键修改）\n",
    "    def parse_response(self, response_text: str) -> Dict:\n",
    "        try:\n",
    "            # 清理响应文本\n",
    "            clean_text = re.sub(r'^```json|```$', '', response_text, flags=re.MULTILINE)\n",
    "            clean_text = re.sub(r',(\\s*?[}\\]])', r'\\1', clean_text)  # 修复尾随逗号\n",
    "            \n",
    "            # 尝试多层解析\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    raw_data = json.loads(clean_text)\n",
    "                    \n",
    "                    # 规范化数据结构\n",
    "                    for table in [\"event_table\", \"interpretability_table\", \"economic_impact_table\"]:\n",
    "                        if isinstance(raw_data.get(table), dict):\n",
    "                            raw_data[table] = [raw_data[table]]\n",
    "                    \n",
    "                    validated = StructuredResponse(**raw_data)\n",
    "                    return validated.model_dump()  # 修改为V2的model_dump()\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # 自动修复常见错误\n",
    "                    if \"Unterminated string\" in str(e):\n",
    "                        clean_text += '\"'\n",
    "                    elif \"Expecting ',' delimiter\" in str(e):\n",
    "                        clean_text = clean_text.rsplit(',', 1)[0] + '}'\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            self.error_handler.add_error(clean_text, \"PARSE_ERROR\", str(e))\n",
    "            return {}\n",
    "\n",
    "    # 其他方法保持不变\n",
    "    def generate_payload(self, text: str) -> dict:\n",
    "        schema_template = json.dumps(StructuredResponse.schema(), indent=2)\n",
    "        return {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt.format(schema_template=schema_template)},\n",
    "                {\"role\": \"user\", \"content\": self.user_prompt.format(text=text)}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 2500\n",
    "        }\n",
    "\n",
    "    def call_api_with_retry(self, text: str, max_retries=3) -> Union[dict, None]:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.post(\n",
    "                    API_URL,\n",
    "                    headers=headers,\n",
    "                    json=self.generate_payload(text),\n",
    "                    timeout=60\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return self.parse_response(response.json()['choices'][0]['message']['content'])\n",
    "            except requests.exceptions.Timeout:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                self.error_handler.add_error(text, \"API_TIMEOUT\", f\"Max retries {max_retries}\")\n",
    "            except Exception as e:\n",
    "                self.error_handler.add_error(text, \"API_ERROR\", str(e))\n",
    "        return None\n",
    "\n",
    "# ------------------ Processing Pipeline ------------------\n",
    "def process_row(processor, row):\n",
    "    # 增加输入预处理（关键修改）\n",
    "    try:\n",
    "        text = str(row['Content']).strip()\n",
    "        if not text:\n",
    "            return {}\n",
    "            \n",
    "        # 生成唯一哈希\n",
    "        text_hash = hashlib.sha256(text.encode()).hexdigest()[:32]\n",
    "        \n",
    "        # 处理时间戳冲突\n",
    "        original_time = f\"{float(row['Time']):.4f}\" if 'Time' in row else \"\"\n",
    "        \n",
    "        response = processor.call_api_with_retry(text)\n",
    "        if not response:\n",
    "            return {}\n",
    "            \n",
    "        response[\"source_meta\"] = {\n",
    "            \"original_time\": original_time,\n",
    "            \"text_hash\": text_hash,\n",
    "            \"raw_text_snippet\": text[:256]\n",
    "        }\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        processor.error_handler.add_error(str(row), \"PROCESS_ERROR\", str(e))\n",
    "        return {}\n",
    "def main_process():\n",
    "    processor = DeepSeekProcessor()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        required_cols = ['Time', 'Content']\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "            \n",
    "        df = df.dropna(subset=['Content']).drop_duplicates(subset=['Content'])\n",
    "        print(f\"Successfully loaded {len(df)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Initialization failed: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(process_row, processor, row) for _, row in df.iterrows()]\n",
    "        \n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            try:\n",
    "                if data := future.result():\n",
    "                    results.append(data)\n",
    "                print(f\"Processed {i+1}/{len(df)}\", end='\\r')\n",
    "            except Exception as e:\n",
    "                processor.error_handler.add_error(\"\", \"THREAD_ERROR\", str(e))\n",
    "\n",
    "    save_results(results, processor.error_handler)\n",
    "    \n",
    "# 保存结果函数保持不变\n",
    "def save_results(data: List[Dict], handler: EnhancedErrorHandler):\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"total_events\": sum(len(d.get(\"event_table\", [])) for d in data),\n",
    "            \"total_interpretations\": sum(len(d.get(\"interpretability_table\", [])) for d in data),\n",
    "            \"economic_impact\": {\n",
    "                \"total_predictions\": sum(len(d.get(\"economic_impact_table\", [])) for d in data),\n",
    "                \"common_indicators\": Counter(\n",
    "                    impact[\"indicator\"] \n",
    "                    for d in data \n",
    "                    for impact in d.get(\"economic_impact_table\", [])\n",
    "                ).most_common(5),\n",
    "                \"confidence_distribution\": {\n",
    "                    \"0-0.3\": sum(1 for d in data for impact in d.get(\"economic_impact_table\", []) if 0 <= impact.get(\"confidence\", 0) <= 0.3),\n",
    "                    \"0.3-0.7\": sum(1 for d in data for impact in d.get(\"economic_impact_table\", []) if 0.3 < impact.get(\"confidence\", 0) <= 0.7),\n",
    "                    \"0.7-1\": sum(1 for d in data for impact in d.get(\"economic_impact_table\", []) if 0.7 < impact.get(\"confidence\", 0) <= 1),\n",
    "                }\n",
    "            },\n",
    "            \"success_rate\": f\"{len(data)/(len(data)+len(handler.error_log)):.1%}\" if (len(data)+len(handler.error_log)) > 0 else \"0%\",\n",
    "            \"error_stats\": {\n",
    "                \"total_errors\": len(handler.error_log),\n",
    "                \"error_types\": dict(Counter(e[\"error_type\"] for e in handler.error_log))\n",
    "            }\n",
    "        },\n",
    "        \"data\": data,\n",
    "        \"errors\": handler.error_log\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main_process()\n",
    "    print(f\"\\nTotal processing time: {time.time()-start_time:.2f}s\")\n",
    "    print(f\"Results saved to: {OUTPUT_JSON}\")"
   ],
   "id": "55cce5cf81bf907d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 298 records\n",
      "Processed 298/298\r\n",
      "Total processing time: 3246.00s\n",
      "Results saved to: enhanced_structured_events.json\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cad283bf814707e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
