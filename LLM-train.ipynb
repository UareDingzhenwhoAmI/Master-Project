{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\"\"\"\n",
    "社会经济事件预测系统 - 完整版\n",
    "包含：数据预处理、时空图神经网络、因果推理模块、训练与可视化\n",
    "严格适配用户提供的JSON数据结构\n",
    "\"\"\"\n",
    "\n",
    "# --------------------- 核心依赖 ---------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import TGCN2, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# --------------------- 日志配置 ---------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('processing.log'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# --------------------- 配置类 ---------------------\n",
    "class Config:\n",
    "    # 数据参数\n",
    "    data_path = \"output_dataset.jsonl\"  # 替换为实际路径\n",
    "    text_embed_model = \"all-mpnet-base-v2\"  # 文本编码模型\n",
    "    geo_default = (39.8283, -98.5795)  # 美国地理中心坐标\n",
    "    \n",
    "    # 经济指标配置\n",
    "    economic_indicators = {\n",
    "        \"Retail Sales\": {\"unit\": \"percent\", \"scaler\": RobustScaler()},\n",
    "        \"Arts Funding\": {\"unit\": \"absolute\", \"scaler\": RobustScaler()},\n",
    "        \"Local GDP\": {\"unit\": \"percent\", \"scaler\": RobustScaler()},\n",
    "        \"Tourism Revenue\": {\"unit\": \"percent\", \"scaler\": RobustScaler()}\n",
    "    }\n",
    "    \n",
    "    # 图模型参数\n",
    "    node_feature_dims = {\n",
    "        \"text_embed\": 768,\n",
    "        \"economic\": 4,\n",
    "        \"temporal\": 6,\n",
    "        \"spatial\": 2,\n",
    "        \"type_embed\": 8\n",
    "    }\n",
    "    hidden_dim = 128\n",
    "    temporal_decay = 0.02\n",
    "    \n",
    "    # 训练参数\n",
    "    epochs = 500\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.0005\n",
    "    early_stopping = 25\n",
    "\n",
    "# --------------------- 数据处理管道 ---------------------\n",
    "class SocioEconomicProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.cfg = config\n",
    "        self.geolocator = Nominatim(user_agent=\"socio_geo_v1\")\n",
    "        self.text_encoder = SentenceTransformer(self.cfg.text_embed_model)\n",
    "        self._init_scalers()\n",
    "        \n",
    "    def _init_scalers(self):\n",
    "        \"\"\"初始化各经济指标的独立标准化器\"\"\"\n",
    "        self.scalers = {\n",
    "            ind: scaler.__class__()  # 创建新实例避免数据泄漏\n",
    "            for ind, scaler in self.cfg.economic_indicators.items()\n",
    "        }\n",
    "    \n",
    "    def full_pipeline(self) -> Data:\n",
    "        \"\"\"端到端数据处理流水线\"\"\"\n",
    "        raw_data = self._load_and_validate()\n",
    "        df = self._parse_to_dataframe(raw_data)\n",
    "        df = self._process_temporal(df)\n",
    "        df = self._process_geospatial(df)\n",
    "        df = self._process_semantic(df)\n",
    "        df = self._process_economic(df)\n",
    "        return self._build_pyg_data(df)\n",
    "    \n",
    "    def _load_and_validate(self) -> dict:\n",
    "        \"\"\"数据加载与完整性验证\"\"\"\n",
    "        if not os.path.exists(self.cfg.data_path):\n",
    "            raise FileNotFoundError(f\"数据文件 {self.cfg.data_path} 不存在\")\n",
    "        \n",
    "        with open(self.cfg.data_path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 结构验证\n",
    "        required_sections = ['event_table', 'interpretability_table', 'economic_impact_table']\n",
    "        for entry in data['data']:\n",
    "            for sec in required_sections:\n",
    "                if sec not in entry:\n",
    "                    raise ValueError(f\"条目缺少必要部分: {sec}\")\n",
    "                if not isinstance(entry[sec], list):\n",
    "                    raise ValueError(f\"{sec} 必须为列表\")\n",
    "        \n",
    "        logging.info(\"数据基础结构验证通过\")\n",
    "        return data\n",
    "    \n",
    "    def _parse_to_dataframe(self, raw_data: dict) -> pd.DataFrame:\n",
    "        \"\"\"将嵌套JSON解析为结构化DataFrame\"\"\"\n",
    "        records = []\n",
    "        \n",
    "        for entry in raw_data['data']:\n",
    "            # 基础事件信息\n",
    "            event_info = {\n",
    "                'event_id': entry['event_table'][0]['event_id'],\n",
    "                'timestamp': pd.to_datetime(entry['event_table'][0]['timestamp'], errors='coerce'),\n",
    "                'event_type': entry['event_table'][0].get('event_type', 'Unknown'),\n",
    "                'location': entry['event_table'][0].get('location', 'Unknown'),\n",
    "                'description': entry['event_table'][0].get('description', ''),\n",
    "                'raw_text': entry['source_meta']['raw_text_snippet']\n",
    "            }\n",
    "            \n",
    "            # 经济影响信息\n",
    "            economic_impacts = {}\n",
    "            for imp in entry['economic_impact_table']:\n",
    "                key = f\"economic_{imp['indicator']}\"\n",
    "                value = self._parse_magnitude(imp['magnitude'], imp['indicator'])\n",
    "                economic_impacts[key] = value\n",
    "            \n",
    "            # 因果路径信息\n",
    "            causal_paths = []\n",
    "            for interp in entry['interpretability_table']:\n",
    "                path = {\n",
    "                    'source': interp['reasoning_path'].split(' → ')[0].strip(),\n",
    "                    'target': interp['reasoning_path'].split(' → ')[-1].strip(),\n",
    "                    'strength': interp['causal_strength'],\n",
    "                    'time_window': interp['time_window']\n",
    "                }\n",
    "                causal_paths.append(path)\n",
    "            \n",
    "            records.append({**event_info, **economic_impacts, 'causal_paths': causal_paths})\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def _parse_magnitude(self, value: str, indicator: str) -> float:\n",
    "        \"\"\"解析不同格式的经济指标值\"\"\"\n",
    "        config = self.cfg.economic_indicators[indicator]\n",
    "        \n",
    "        try:\n",
    "            # 百分比处理\n",
    "            if config['unit'] == \"percent\" and '%' in value:\n",
    "                return float(value.strip('%')) / 100\n",
    "            # 绝对数值处理\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            logging.warning(f\"无法解析指标值: {value}，已替换为0\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _process_temporal(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"时间特征工程\"\"\"\n",
    "        # 基础时间特征\n",
    "        df['days'] = (df['timestamp'] - pd.Timestamp('2000-01-01')).dt.days\n",
    "        \n",
    "        # 周期性编码\n",
    "        df['year_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.year / 2024)\n",
    "        df['year_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.year / 2024)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.day / 31)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.day / 31)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _process_geospatial(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"地理空间处理\"\"\"\n",
    "        # 地理编码\n",
    "        df['coordinates'] = df['location'].apply(self._geocode_location)\n",
    "        \n",
    "        # 区域划分\n",
    "        df['region'] = df['coordinates'].apply(\n",
    "            lambda x: self._determine_region(x, self.cfg.geo_default))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _geocode_location(self, loc_str: str) -> Tuple[float, float]:\n",
    "        \"\"\"分级地理编码策略\"\"\"\n",
    "        try:\n",
    "            # 尝试完整地址\n",
    "            loc = self.geolocator.geocode(loc_str, timeout=10)\n",
    "            if loc: return (loc.latitude, loc.longitude)\n",
    "            \n",
    "            # 分层解析\n",
    "            parts = [p.strip() for p in loc_str.split('/') if p.strip()]\n",
    "            for i in range(len(parts)-1, 0, -1):\n",
    "                loc = self.geolocator.geocode(\"/\".join(parts[i:]), timeout=5)\n",
    "                if loc: return (loc.latitude, loc.longitude)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"地理编码失败: {loc_str} - {str(e)}\")\n",
    "        \n",
    "        return self.cfg.geo_default\n",
    "    \n",
    "    def _determine_region(self, coords: Tuple[float, float], default) -> int:\n",
    "        \"\"\"划分地理区域\"\"\"\n",
    "        # 示例：简单经纬度划分\n",
    "        lat, lon = coords\n",
    "        if lat > 40: return 0  # 北部\n",
    "        elif lat < 35: return 1  # 南部\n",
    "        return 2 if lon < -100 else 3  # 西部/东部\n",
    "    \n",
    "    def _process_semantic(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"语义特征处理\"\"\"\n",
    "        # 拼接文本\n",
    "        texts = df.apply(lambda x: f\"{x['event_type']}: {x['description']} [CONTEXT] {x['raw_text']}\", axis=1)\n",
    "        \n",
    "        # 批量编码\n",
    "        df['text_embed'] = self.text_encoder.encode(texts.tolist(), show_progress_bar=True, batch_size=32)\n",
    "        return df\n",
    "    \n",
    "    def _process_economic(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"经济指标处理\"\"\"\n",
    "        # 独立标准化\n",
    "        for ind in self.cfg.economic_indicators:\n",
    "            col = f\"economic_{ind}\"\n",
    "            scaler = self.scalers[ind]\n",
    "            df[col] = scaler.fit_transform(df[[col]].values)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _build_pyg_data(self, df: pd.DataFrame) -> Data:\n",
    "        \"\"\"构建PyG数据对象\"\"\"\n",
    "        # 节点特征\n",
    "        features = {\n",
    "            'text': np.stack(df['text_embed']),\n",
    "            'economic': df[[c for c in df.columns if c.startswith('economic_')]].values,\n",
    "            'temporal': df[['year_sin', 'year_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos']].values,\n",
    "            'spatial': np.array([list(x) for x in df['coordinates']]),\n",
    "            'type': pd.get_dummies(df['event_type']).values\n",
    "        }\n",
    "        x = np.hstack([features['text'], features['economic'], features['temporal'], \n",
    "                      features['spatial'], features['type']])\n",
    "        \n",
    "        # 边构建\n",
    "        edge_index, edge_attr = [], []\n",
    "        event_id_map = df['event_id'].reset_index().set_index('event_id')['index'].to_dict()\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            for path in row['causal_paths']:\n",
    "                src = event_id_map.get(path['source'], -1)\n",
    "                tgt = event_id_map.get(path['target'], -1)\n",
    "                \n",
    "                if src == -1 or tgt == -1:\n",
    "                    logging.warning(f\"无效因果路径: {path['source']} → {path['target']}\")\n",
    "                    continue\n",
    "                \n",
    "                # 时间衰减计算\n",
    "                time_diff = df.iloc[tgt]['days'] - df.iloc[src]['days']\n",
    "                decay = np.exp(-self.cfg.temporal_decay * abs(time_diff))\n",
    "                weight = path['strength'] * decay\n",
    "                \n",
    "                edge_index.append([src, tgt])\n",
    "                edge_attr.append(weight)\n",
    "        \n",
    "        return Data(\n",
    "            x=torch.tensor(x, dtype=torch.float32),\n",
    "            edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
    "            edge_attr=torch.tensor(edge_attr, dtype=torch.float32).unsqueeze(1),\n",
    "            y=torch.tensor(df[[c for c in df.columns if c.startswith('economic_')]].values, \n",
    "                     dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# --------------------- 因果增强GNN模型 ---------------------\n",
    "class CausalTGCN(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        \n",
    "        # 时序图卷积\n",
    "        self.tgcn = TGCN2(\n",
    "            in_channels=self._calc_input_dim(),\n",
    "            out_channels=self.cfg.hidden_dim\n",
    "        )\n",
    "        \n",
    "        # 动态边权重\n",
    "        self.edge_weight_layer = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 预测头\n",
    "        self.pred_head = nn.Sequential(\n",
    "            nn.Linear(self.cfg.hidden_dim, self.cfg.hidden_dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.cfg.hidden_dim//2, len(self.cfg.economic_indicators))\n",
    "        )\n",
    "    \n",
    "    def _calc_input_dim(self) -> int:\n",
    "        \"\"\"计算总输入维度\"\"\"\n",
    "        return sum(self.cfg.node_feature_dims.values())\n",
    "    \n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        # 动态边权重\n",
    "        edge_weights = self.edge_weight_layer(data.edge_attr)\n",
    "        \n",
    "        # 时空卷积\n",
    "        h = self.tgcn(data.x, data.edge_index, edge_weights)\n",
    "        \n",
    "        # 全局池化\n",
    "        graph_embed = global_mean_pool(h, batch=None)\n",
    "        \n",
    "        # 经济指标预测\n",
    "        return self.pred_head(graph_embed)\n",
    "\n",
    "# --------------------- 训练框架 ---------------------\n",
    "class SocioEconomicTrainer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.cfg = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 初始化组件\n",
    "        self.processor = SocioEconomicProcessor(config)\n",
    "        self.model = CausalTGCN(config).to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, 'min', patience=5\n",
    "        )\n",
    "    \n",
    "    def execute(self):\n",
    "        \"\"\"执行完整训练流程\"\"\"\n",
    "        try:\n",
    "            dataset = self.processor.full_pipeline()\n",
    "            self._train(dataset)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"训练流程异常终止: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _train(self, dataset: Data):\n",
    "        \"\"\"训练循环\"\"\"\n",
    "        # 时间序列分割\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(dataset.x)):\n",
    "            logging.info(f\"\\n=== 开始第 {fold+1}/5 折交叉验证 ===\")\n",
    "            \n",
    "            # 数据准备\n",
    "            train_data = dataset[train_idx]\n",
    "            val_data = dataset[val_idx]\n",
    "            \n",
    "            train_loader = DataLoader([train_data], batch_size=1, shuffle=False)\n",
    "            val_loader = DataLoader([val_data], batch_size=1)\n",
    "            \n",
    "            # 折叠训练\n",
    "            for epoch in range(self.cfg.epochs):\n",
    "                self.model.train()\n",
    "                train_loss = 0.0\n",
    "                \n",
    "                for batch in train_loader:\n",
    "                    batch = batch.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    pred = self.model(batch)\n",
    "                    loss = F.huber_loss(pred, batch.y, delta=1.0)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # 梯度裁剪\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # 验证阶段\n",
    "                val_loss = self._validate(val_loader)\n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "                # 早停检查\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(self.model.state_dict(), f\"best_model_fold{fold}.pth\")\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    if patience >= self.cfg.early_stopping:\n",
    "                        logging.info(f\"早停触发于第 {epoch} 轮\")\n",
    "                        break\n",
    "                \n",
    "                # 日志记录\n",
    "                logging.info(\n",
    "                    f\"Epoch {epoch+1}/{self.cfg.epochs} | \"\n",
    "                    f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "                    f\"Val Loss: {val_loss:.4f} | \"\n",
    "                    f\"LR: {self.optimizer.param_groups[0]['lr']:.2e}\"\n",
    "                )\n",
    "    \n",
    "    def _validate(self, loader: DataLoader) -> float:\n",
    "        \"\"\"验证循环\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(self.device)\n",
    "                pred = self.model(batch)\n",
    "                total_loss += F.huber_loss(pred, batch.y).item()\n",
    "        \n",
    "        return total_loss / len(loader)\n",
    "\n",
    "# --------------------- 可视化模块 ---------------------\n",
    "class SocioVisualizer:\n",
    "    @staticmethod\n",
    "    def plot_causal_graph(data: Data, top_n=50):\n",
    "        \"\"\"绘制因果图\"\"\"\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # 添加节点\n",
    "        for i in range(min(len(data.x), top_n)):\n",
    "            node_type = torch.argmax(data.x[i][-8:]).item()\n",
    "            G.add_node(i, type=node_type)\n",
    "        \n",
    "        # 添加边\n",
    "        edge_list = data.edge_index.t().tolist()[:top_n*2]\n",
    "        for (s, t), w in zip(edge_list, data.edge_attr[:top_n*2]):\n",
    "            if s < top_n and t < top_n:\n",
    "                G.add_edge(s, t, weight=w.item())\n",
    "        \n",
    "        # 可视化参数\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "        node_colors = [G.nodes[n]['type'] for n in G.nodes]\n",
    "        edge_weights = [G.edges[e]['weight']*2 for e in G.edges]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.tab20, node_size=500)\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_weights, edge_color=edge_weights, \n",
    "                              edge_cmap=plt.cm.Blues, arrows=True)\n",
    "        nx.draw_networkx_labels(G, pos)\n",
    "        \n",
    "        plt.title(\"Top 50 因果事件网络\")\n",
    "        plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Blues), label=\"因果强度\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_economic_trends(predictions: np.ndarray, \n",
    "                            ground_truth: np.ndarray,\n",
    "                            indicators: List[str]):\n",
    "        \"\"\"经济指标预测趋势可视化\"\"\"\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        for i, ind in enumerate(indicators):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.plot(ground_truth[:, i], label='实际值', color='blue', alpha=0.6)\n",
    "            plt.plot(predictions[:, i], label='预测值', color='red', linestyle='--')\n",
    "            plt.title(ind)\n",
    "            plt.xlabel(\"时间步\")\n",
    "            plt.ylabel(\"标准化值\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --------------------- 主程序 ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化系统\n",
    "    cfg = Config()\n",
    "    trainer = SocioEconomicTrainer(cfg)\n",
    "    \n",
    "    try:\n",
    "        # 执行完整流程\n",
    "        trainer.execute()\n",
    "        \n",
    "        # 可视化结果\n",
    "        dataset = SocioEconomicProcessor(cfg).full_pipeline()\n",
    "        SocioVisualizer.plot_causal_graph(dataset)\n",
    "        \n",
    "        # 加载最佳模型预测\n",
    "        model = CausalTGCN(cfg).to(trainer.device)\n",
    "        model.load_state_dict(torch.load(\"best_model_fold0.pth\"))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = model(dataset.to(trainer.device)).cpu().numpy()\n",
    "            true = dataset.y.numpy()\n",
    "            SocioVisualizer.plot_economic_trends(pred, true, list(cfg.economic_indicators.keys()))\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"系统运行异常: {str(e)}\")\n",
    "        raise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"PyTorch 版本:\", torch.__version__)          # 应输出 2.3.0+cu121\n",
    "print(\"CUDA 是否可用:\", torch.cuda.is_available())  # 应输出 True\n",
    "\n",
    "import torch_geometric\n",
    "print(\"PyG 版本:\", torch_geometric.__version__)    # 应输出 2.5.0"
   ],
   "id": "fc814ba6d6b1966f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49103af5fc643666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
